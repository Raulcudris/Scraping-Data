{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 2: HTML: Requests y BeautifulSoup\n",
    "## Parsing Pagina12\n",
    "\n",
    "<img src='https://www.pagina12.com.ar/assets/media/logos/logo_pagina_12_n.svg?v=1.0.178' width=300></img>\n",
    "En este módulo veremos cómo utilizar las bibliotecas `requests` y `bs4` para programar scrapers de sitios HTML. Nos propondremos armar un scraper de noticias del diario <a href='www.pagina12.com.ar'>Página 12</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos leer el diario por internet. Lo primero que hacemos es abrir el navegador, escribir la URL del diario y apretar Enter para que aparezca la página del diario. Lo que ocurre en el momento en el que apretamos Enter es lo siguiente:\n",
    "1. El navegador envía una solicitud a la URL pidiéndole información.\n",
    "2. El servidor recibe la petición y procesa la respuesta.\n",
    "3. El servidor envía la respuesta a la IP de la cual recibió la solicitud.\n",
    "4. Nuestro navegador recibe la respuesta y la muestra **formateada** en pantalla.\n",
    "\n",
    "Para hacer un scraper debemos hacer un programa que replique este flujo de forma automática para luego extraer la información deseada de la respuesta. Utilizaremos `requests` para realizar peticiones y recibir las respuestas y `bs4` para *parsear* la respuesta y extraer la información.<br>\n",
    "Te dejo unos links que tal vez te sean de utilidad:\n",
    "- [Códigos de status HTTP](https://developer.mozilla.org/es/docs/Web/HTTP/Status)\n",
    "- [Documentación de requests](https://requests.kennethreitz.org/en/master/)\n",
    "- [Documentación de bs4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pagina12.com.ar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12 = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas veces la respuesta a la solicitud puede ser algo que no sea un texto: una imagen, un archivo de audio, un video, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analicemos otros elementos de la respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El contenido de la request que acabamos de hacer está avisando que estamos utilizando la biblioteca requests para python y que no es un navegador convencional. Se puede modificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p12.cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = BeautifulSoup(p12.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer ejercicio: obtener un listado de links a las distintas secciones del diario.<br>\n",
    "Usar el inspector de elementos para ver dónde se encuentra la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secciones = s.find('ul', attrs={'class':'hot-sections'}).find_all('li')\n",
    "secciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[seccion.text for seccion in secciones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seccion = secciones[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seccion.a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos interesados en los links, no en el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_secciones = [seccion.a.get('href') for seccion in secciones]\n",
    "links_secciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos la página de una sección para ver cómo se compone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = requests.get(links_secciones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec.request.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_seccion = BeautifulSoup(sec.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_seccion.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La página se divide en un artículo promocionado y una lista `<ul>` con el resto de los artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_article = soup_seccion.find('div', attrs={'class':'featured-article__container'})\n",
    "featured_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_article.a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = soup_seccion.find('ul', attrs={'class':'article-list'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_notas(soup):\n",
    "    '''\n",
    "    Función que recibe un objeto de BeautifulSoup de una página de una sección\n",
    "    y devuelve una lista de URLs a las notas de esa sección\n",
    "    '''\n",
    "    lista_notas = []\n",
    "    \n",
    "    # Obtengo el artículo promocionado\n",
    "    featured_article = soup.find('div', attrs={'class':'featured-article__container'})\n",
    "    if featured_article:\n",
    "        lista_notas.append(featured_article.a.get('href'))\n",
    "    \n",
    "    # Obtengo el listado de artículos\n",
    "    article_list = soup.find('ul', attrs={'class':'article-list'})\n",
    "    for article in article_list.find_all('li'):\n",
    "        if article.a:\n",
    "            lista_notas.append(article.a.get('href'))\n",
    "    \n",
    "    return lista_notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_notas = obtener_notas(soup_seccion)\n",
    "lista_notas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(lista_notas[0])\n",
    "if r.status_code == 200:\n",
    "    # Procesamos la respuesta\n",
    "    print('procesamos..')\n",
    "else:\n",
    "     # Informar el error\n",
    "    print('informamos...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_nota = lista_notas[0]\n",
    "print(url_nota)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_mala = url_nota.replace('2','3')\n",
    "print(url_mala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nota = requests.get(url_mala)\n",
    "except Exception as e:\n",
    "    print('Error en la request:')\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    \n",
    "print('El resto del programa continúa...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nota = requests.get(url_nota)\n",
    "    if nota.status_code == 200:\n",
    "        s_nota = BeautifulSoup(nota.text, 'lxml')\n",
    "        # Extraemos el título\n",
    "        titulo = s_nota.find('div', attrs={'class':'article-title'})\n",
    "        print(titulo.text)\n",
    "        # Extraemos la fecha\n",
    "        fecha = s_nota.find('span', attrs={'pubdate':'pubdate'}).get('datetime')\n",
    "        print(fecha)\n",
    "        \n",
    "except Exception as e:\n",
    "    print('Error en la request:')\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = s_nota.find('div', attrs={'class':'article-main-media-image'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenes = media.find_all('img')\n",
    "imagenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtuvimos varias imágenes de distintos tamaños que se mostrarán en función del tamaño de la pantalla/navegador. Vemos que están ordenadas por tamaño así que intentaremos obtener la de mayor resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(imagenes) == 0:\n",
    "    print('no se encontraron imágenes')\n",
    "else:\n",
    "    imagen = imagenes[-1]\n",
    "    img_src = imagen.get('data-src')\n",
    "    print(img_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_req = requests.get(img_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_req.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_req.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(img_req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_info(s_nota):\n",
    "    \n",
    "    # Creamos un diccionario vacío para poblarlo con la información\n",
    "    ret_dict = {}\n",
    "    \n",
    "    # Extraemos la fecha\n",
    "    fecha = s_nota.find('span', attrs={'pubdate':'pubdate'})\n",
    "    if fecha:\n",
    "        ret_dict['fecha'] = fecha.get('datetime')\n",
    "    else:\n",
    "        ret_dict['fecha'] = None\n",
    "    \n",
    "    # Extraemos el título\n",
    "    titulo = s_nota.find('div', attrs={'class':'article-title'})\n",
    "    if titulo:\n",
    "        ret_dict['titulo'] = titulo.text\n",
    "    else:\n",
    "        ret_dict['titulo'] = None\n",
    "\n",
    "    # Extraemos la volanta\n",
    "    volanta = s_nota.find('div', attrs={'class':'article-prefix'})\n",
    "    if volanta:\n",
    "        ret_dict['volanta'] = volanta.get_text()\n",
    "    else:\n",
    "        ret_dict['volanta'] = None\n",
    "    \n",
    "    # Extraemos el copete\n",
    "    copete = s_nota.find('div', attrs={'class':'article-summary'})\n",
    "    if copete:\n",
    "        ret_dict['copete'] = volanta.get_text()\n",
    "    else:\n",
    "        ret_dict['copete'] = None\n",
    "    \n",
    "    autor = s_nota.find('div', attrs={'class':'article-author'})\n",
    "    if autor:\n",
    "        ret_dict['autor'] = autor.a.get_text()\n",
    "    else:\n",
    "        ret_dict['autor'] = None\n",
    "    \n",
    "    # Extraemos la imagen\n",
    "    media = s_nota.find('div', attrs={'class':'article-main-media-image'})\n",
    "    if media:\n",
    "        imagenes = media.find_all('img')\n",
    "        if len(imagenes) == 0:\n",
    "            print('no se encontraron imágenes')\n",
    "        else:\n",
    "            imagen = imagenes[-1]\n",
    "            img_src = imagen.get('data-src')\n",
    "            try:\n",
    "                img_req = requests.get(img_src)\n",
    "                if img_req.status_code == 200:\n",
    "                    ret_dict['imagen'] = img_req.content\n",
    "                else:\n",
    "                    ret_dict['imagen'] = None\n",
    "            except:\n",
    "                print('No se pudo obtener la imagen')\n",
    "    else:\n",
    "        print('No se encontró media')\n",
    "    \n",
    "    # Extraemos el cuerpo de la nota\n",
    "    cuerpo = s_nota.find('div', attrs={'class':'article-text'})\n",
    "    if cuerpo:\n",
    "        ret_dict['texto'] = cuerpo.get_text()\n",
    "    else:\n",
    "        ret_dict['texto'] = None\n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase 6\n",
    "En esta clase vamos a utilizar todo lo aprendido para armar el scraper.\n",
    "Vamos a definir una función que a partir de la URL de una nota, devuelva un diccionario con toda su información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "platzi",
   "language": "python",
   "name": "platzi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
